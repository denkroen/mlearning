{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature exploring and engineering\n",
    "\n",
    "In the first steps we explored which features are provided by the dataset and how they interact with eachother. One important feature group for us were the wind features. We found out, that wind is cooling down solar panels during the day and thus increasing their efficiency. Furthermore we found some articles that elaborate on the impact of the wind direction on the performance on solar panels and that it can be derived from the measured u and v components. So we decided to create the wind direction feature and also to estimate the wind speed with the u and v component. The calculated windspeed did not match up with the provided measured windspeed (possibly because of measurement inaccuracys) but it still increased the performance of our model.\n",
    "\n",
    "We also explored a group of sun features (sun_azimuth and sun_elevation). With these features and information about the tilt angle and location of the solar panel, it is possible to derive information about the energy capture. Since we dont have information about the position or the tilt of the solar panel, we scraped the idea in the beginning.\n",
    "\n",
    "During testing of our model, we also found out, that the features related to snow where harmful for the predictions. Therefore we removed them in th beginning and iteratively determined usefull snow features for the different locations.\n",
    "We also decided to remove constant features and features with a low amount of unique values like the \"wind speed w\" and the \"elevation\", since they dont give us any relevant information.\n",
    "\n",
    "Since we try to predict estimated data, we use a part of estimated_train as validation and a part of it in training. Since the data is more valuable for the predictions, we tried to increase the the significance of the data. First we removed some observed data to increase the percantage of the valdation data during training, but this decreased the performance. We then used a flag for the data wich indicates if the data is estimated or not.\n",
    "\n",
    "Furthermore we looked into different individual features of the dataset that were not intuitive for us. We researched on how they impact the solar energy production in the documentation and the internet:\n",
    "\n",
    "Supercooled Liquid Water refers to liquid water droplets below the freezing point and can reduce the transmission of sunlight. It can be used to predict icing events.\n",
    "\n",
    "visibility gives information about fog. If the visibility is below 1 km, the weather is declared as foggy. This is the reason we scaled it to kilometers for our model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['dew_or_rime:idx', 'elevation:m', 'is_day:idx', 'is_in_shadow:idx',\n",
      "       'precip_type_5min:idx', 'snow_density:kgm3', 'snow_drift:idx',\n",
      "       'wind_speed_w_1000hPa:ms'],\n",
      "      dtype='object')\n",
      "Index(['dew_or_rime:idx', 'elevation:m', 'is_day:idx', 'is_in_shadow:idx',\n",
      "       'precip_type_5min:idx', 'rain_water:kgm2', 'snow_density:kgm3',\n",
      "       'snow_drift:idx', 'wind_speed_w_1000hPa:ms'],\n",
      "      dtype='object')\n",
      "Index(['dew_or_rime:idx', 'elevation:m', 'fresh_snow_12h:cm',\n",
      "       'fresh_snow_1h:cm', 'fresh_snow_24h:cm', 'fresh_snow_3h:cm',\n",
      "       'fresh_snow_6h:cm', 'is_day:idx', 'is_in_shadow:idx',\n",
      "       'precip_type_5min:idx', 'rain_water:kgm2', 'snow_density:kgm3',\n",
      "       'snow_depth:cm', 'snow_drift:idx', 'snow_melt_10min:mm',\n",
      "       'super_cooled_liquid_water:kgm2', 'wind_speed_w_1000hPa:ms'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "# from Preprocessing import *\n",
    "\n",
    "''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "Preprocessing functions\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "def drop_consecutive_nonzero_repeats(df):\n",
    "    count = 0\n",
    "    previous_value = None\n",
    "    indices_to_drop = []\n",
    "\n",
    "    for i, value in enumerate(df['pv_measurement']):\n",
    "        if value != 0:  # Exclude zeros\n",
    "            if value == previous_value:\n",
    "                count += 1\n",
    "                if count > 5:  # 3 consecutive times the same number\n",
    "                    # Mark the indices to be dropped\n",
    "                    indices_to_drop.extend(list(range(i - count, i + 1)))\n",
    "            else:\n",
    "                count = 0\n",
    "            previous_value = value\n",
    "\n",
    "    # Drop the rows with consecutive repeats\n",
    "    df = df.drop(indices_to_drop)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocessing(df,target,soort_data,id):\n",
    "    \n",
    "        target.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "        \n",
    "        # 1. Attempt to drop the 'date_calc' column (if it exists) in each DataFrame\n",
    "        if 'date_calc' in df.columns:\n",
    "            df = df.drop('date_calc', axis=1)\n",
    "\n",
    "        #print(df.head())\n",
    "\n",
    "        imputer = IterativeImputer()\n",
    "\n",
    "        # Select the numeric columns for imputation\n",
    "        columns_to_impute = ['ceiling_height_agl:m']\n",
    "\n",
    "        # Perform imputation\n",
    "        df[columns_to_impute] = imputer.fit_transform(df[columns_to_impute])\n",
    "\n",
    "        #df['cloud_base_agl:m'].interpolate(method='linear', inplace=True)#.fillna(method='ffill', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if id == 'a':\n",
    "            df = df.drop(\"wind_speed_w_1000hPa:ms\", axis=1)\n",
    "            df[\"visibility:m\"] = df[\"visibility:m\"] * 1000\n",
    "            df = df.drop(\"elevation:m\", axis=1)\n",
    "            df = df.drop(\"precip_5min:mm\", axis=1) \n",
    "            df = df.drop(\"effective_cloud_cover:p\", axis=1) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            filter1 = [col for col in df.columns if 'snow' not in col or col in ['fresh_snow_24h:cm', 'fresh_snow_12h:cm']]\n",
    "            df = df[filter1]\n",
    "\n",
    "        if id == \"b\":\n",
    "\n",
    "           \n",
    "\n",
    "            df = df.drop(\"wind_speed_w_1000hPa:ms\", axis=1)\n",
    "            df = df.drop(\"elevation:m\", axis=1)\n",
    "            df[\"visibility:m\"] = df[\"visibility:m\"] * 1000\n",
    "\n",
    "\n",
    "            filter1 = [col for col in df.columns if 'snow' not in col or col in ['fresh_snow_24h:cm', 'fresh_snow_12h:cm']]\n",
    "            df = df[filter1]\n",
    "\n",
    "\n",
    "        if id == \"c\":\n",
    "              # Determine columns with less than 5 unique values\n",
    "    \n",
    "            columns_with_few_unique_values = df.columns[df.nunique() < 8]\n",
    "\n",
    "            # Print the names of the columns meeting the condition\n",
    "            print(columns_with_few_unique_values)\n",
    "            df = df.drop(\"elevation:m\", axis=1)\n",
    "            df[\"visibility:m\"] = df[\"visibility:m\"] * 1000\n",
    "\n",
    "            df = df.drop(\"wind_speed_w_1000hPa:ms\", axis=1)\n",
    "            filter1 = [col for col in df.columns if 'snow' not in col or col in ['fresh_snow_24h:cm', 'fresh_snow_12h:cm', \"snow_drift:idx\"]]\n",
    "            df = df[filter1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "                \n",
    "    \n",
    "\n",
    "       # 2. Linear interpolation for all columns\n",
    "        #for features in df.columns:\n",
    "        #    if features == 'snow_density:kgm3':\n",
    "        #        df[features].fillna(0, inplace=True)\n",
    "        #    else:\n",
    "                # Interpolate missing values using linear interpolation\n",
    "        #        df[features] = df[features].interpolate(method='linear')\n",
    "        #print(df.shape)\n",
    "        \n",
    "        # # 3. Remove columns with constant data\n",
    "        #unique_counts = df.nunique()\n",
    "        #constant_features = unique_counts[unique_counts == 1].index\n",
    "        #df = df.drop(columns=constant_features, axis=1)\n",
    "\n",
    "        \n",
    "\n",
    "        # 4. Set 'date_forecast' as the index and resample to hourly data\n",
    "\n",
    "        df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "        df.set_index('date_forecast', inplace=True)\n",
    "        df = df.resample('H').mean()\n",
    "        #print(df.shape)\n",
    "\n",
    "        # 5. Merge the first and second DataFrames in the preprocessed_features list \n",
    "        # 6. Merge the first and third DataFrames in the input_features list\n",
    "\n",
    "        if soort_data == 'train_observed' or soort_data == 'train_estimated':\n",
    "            df = pd.merge(df, target, on='date_forecast', how='inner')\n",
    "\n",
    "        # 7. Conditional operations for 'pv_measurement' column (if it exists).\n",
    "        if 'pv_measurement' in df.columns:\n",
    "            # Define and use functions to drop consecutive non-zero and zero repeats (not provided)\n",
    "            df = drop_consecutive_nonzero_repeats(df)\n",
    "            #df = drop_consecutive_zero_repeats(df)\n",
    "            #df = df.fillna(0)\n",
    "            \n",
    "         # 8. Attempt to drop the 'date_forecast' column (if it exists) in each DataFrame\n",
    "        if 'date_forecast' in df.columns:\n",
    "            df = df.drop('date_forecast', axis=1)\n",
    "            \n",
    "        # Drop all rows where all columns are empty\n",
    "        if soort_data == 'train_observed' or soort_data == 'train_estimated':\n",
    "            if id == \"a\":\n",
    "                df = df.fillna(0)#dropna(how='any')\n",
    "            else:\n",
    "                df = df.fillna(0)\n",
    "        else:\n",
    "            df = df.dropna(how='all')\n",
    "            #df = df.fillna(0)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "Functions\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "#define function to load all the data\n",
    "def load_data(location):\n",
    "    target = pd.read_parquet(f'{location}/raw/train_targets.parquet')\n",
    "    train_observed = pd.read_parquet(f'{location}/raw/X_train_observed.parquet')\n",
    "    train_estimated = pd.read_parquet(f'{location}/raw/X_train_estimated.parquet')\n",
    "    test_estimated = pd.read_parquet(f'{location}/raw/X_test_estimated.parquet')\n",
    "    \n",
    "    #put all the data of one location into a list\n",
    "    data = [target, train_observed, train_estimated, test_estimated]\n",
    "    return data\n",
    "\n",
    "\n",
    "#preprocess the three different datasets for all locations\n",
    "def preprocess_data(data,id):\n",
    "    train_observed = preprocessing(data[1],data[0],'train_observed',id)\n",
    "    train_estimated = preprocessing(data[2],data[0],'train_estimated',id)\n",
    "    test_estimated = preprocessing(data[3],data[0],'test_estimated',id)\n",
    "    data = [train_observed, train_estimated, test_estimated]\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_to_file(data_to_file,location):\n",
    "    #saving train estimated data to csv\n",
    "    data_to_file[0].to_csv(f'{location}/preproc_train_observed_{location}.csv', index=False)\n",
    "    data_to_file[1].to_csv(f'{location}/preproc_train_estimated_{location}.csv', index=False)\n",
    "    data_to_file[2].to_csv(f'{location}/preproc_test_estimated_{location}.csv', index=False)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "Main pipline\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "\n",
    "\n",
    "#load all the data and put them in a separate list for every location\n",
    "data_A = load_data('A')\n",
    "data_B = load_data('B')\n",
    "data_C = load_data('C')\n",
    "\n",
    "preprocessed_A = preprocess_data(data_A,\"a\")\n",
    "preprocessed_B = preprocess_data(data_B,\"b\")\n",
    "preprocessed_C = preprocess_data(data_C,\"c\")\n",
    "\n",
    "#save preprocessed data\n",
    "preprocessed_A = save_to_file(preprocessed_A,'A')\n",
    "preprocessed_B = save_to_file(preprocessed_B,'B')\n",
    "preprocessed_C = save_to_file(preprocessed_C,'C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "Functions\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "def create_estimated(df, id):\n",
    "    df[\"is_estimated\"] = pd.Series([id]*len(df), name=\"is_estimated\")\n",
    "\n",
    "def create_wind(df):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    df[\"windSpeed\"] = np.sqrt(df[\"wind_speed_u_10m:ms\"]**2 + df[\"wind_speed_v_10m:ms\"]**2)\n",
    "    df[\"windAngle\"] = np.arctan2(df[\"wind_speed_v_10m:ms\"], df[\"wind_speed_u_10m:ms\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_submission(predictions_A,predictions_B,predictions_C):\n",
    "\n",
    "    combined_predictions = pd.concat([predictions_A,predictions_B,predictions_C], axis=0)\n",
    "    combined_predictions = combined_predictions.reset_index(drop=True)\n",
    "    combined_predictions.index.name = 'id'\n",
    "    combined_predictions.rename('prediction', inplace=True)\n",
    "    combined_predictions.to_csv('auto_predictions_finalv31_bare_50_80.csv')\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "train_data = TabularDataset('A/preproc_train_observed_A.csv')\n",
    "train_data.head()\n",
    "label = 'pv_measurement'\n",
    "test_data = TabularDataset(f'A/preproc_train_estimated_A.csv')\n",
    "\n",
    "#transform date\n",
    "train_data['date_forecast'] = pd.to_datetime(train_data.date_forecast, format='%Y-%m-%d %H:%M:%S')\n",
    "test_data['date_forecast'] = pd.to_datetime(test_data.date_forecast, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "train_data = create_estimated(train_data,0)\n",
    "test_data = create_estimated(test_data,1)\n",
    "\n",
    "train_data = create_wind(train_data)\n",
    "test_data = create_wind(test_data)\n",
    "\n",
    "\n",
    "#Add validation data to training weather is_est\n",
    "percentage = 0.50\n",
    "num_rows = int(len(test_data) * percentage)\n",
    "sample = test_data.sample(n=num_rows, random_state=42)\n",
    "test_data = test_data.drop(sample.index)\n",
    "sample[\"is_estimated\"] = pd.Series([1] * len(sample), name='is_estimated')\n",
    "train_data[\"is_estimated\"] = pd.Series([0] * len(train_data), name='is_estimated')\n",
    "\n",
    "train_data = pd.concat([train_data,sample]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#Train and predict\n",
    "predictor = TabularPredictor(label=label, eval_metric='mean_absolute_error').fit(train_data, tuning_data=test_data)\n",
    "\n",
    "#only for analysis. Takes some time to compute.\n",
    "#x = predictor.feature_importance(test_data)\n",
    "#print(x)\n",
    "\n",
    "\n",
    "submission_data = TabularDataset('A/preproc_test_estimated_A.csv')\n",
    "submission_data['date_forecast'] = pd.to_datetime(submission_data.date_forecast, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "submission_data = create_wind(submission_data)\n",
    "submission_data = delete_snow(submission_data)\n",
    "\n",
    "\n",
    "\n",
    "predictions_A = predictor.predict(submission_data)\n",
    "\n",
    "##########B\n",
    "\n",
    "train_data = TabularDataset('B/preproc_train_observed_B.csv')\n",
    "label = 'pv_measurement'\n",
    "test_data = TabularDataset(f'B/preproc_train_estimated_B.csv')\n",
    "\n",
    "\n",
    "train_data = create_estimated(train_data,0)\n",
    "test_data = create_estimated(test_data,1)\n",
    "\n",
    "train_data = create_wind(train_data)\n",
    "test_data = create_wind(test_data)\n",
    "\n",
    "#Add validation data to training\n",
    "percentage = 0.50\n",
    "num_rows = int(len(test_data) * percentage)\n",
    "sample = test_data.sample(n=num_rows, random_state=42)\n",
    "test_data = test_data.drop(sample.index)\n",
    "train_data = pd.concat([train_data,sample]).reset_index(drop=True)\n",
    "\n",
    "predictor = TabularPredictor(label=label, eval_metric='mean_absolute_error').fit(train_data, presets='best_quality', ag_args_fit={'num_gpus':1}, num_stack_levels=0,tuning_data=test_data,use_bag_holdout=True)\n",
    "x = predictor.feature_importance(test_data)\n",
    "\n",
    "print(x)\n",
    "\n",
    "submission_data = TabularDataset('B/preproc_test_estimated_B.csv')\n",
    "\n",
    "predictions_B = predictor.predict(submission_data)\n",
    "\n",
    "###########C\n",
    "\n",
    "train_data = TabularDataset('C/preproc_train_observed_C.csv')\n",
    "label = 'pv_measurement'\n",
    "test_data = TabularDataset(f'C/preproc_train_estimated_C.csv')\n",
    "\n",
    "\n",
    "train_data = create_estimated(train_data,0)\n",
    "test_data = create_estimated(test_data,1)\n",
    "\n",
    "train_data = create_wind(train_data)\n",
    "test_data = create_wind(test_data)\n",
    "\n",
    "percentage = 0.50\n",
    "num_rows = int(len(test_data) * percentage)\n",
    "sample = test_data.sample(n=num_rows, random_state=42)\n",
    "test_data = test_data.drop(sample.index)\n",
    "train_data = pd.concat([train_data,sample]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "predictor = TabularPredictor(label=label, eval_metric='mean_absolute_error').fit(train_data, presets='best_quality', ag_args_fit={'num_gpus':1}, num_stack_levels=0,tuning_data=test_data,use_bag_holdout=True)\n",
    "x = predictor.feature_importance(test_data)\n",
    "\n",
    "print(x)\n",
    "\n",
    "submission_data = TabularDataset('C/preproc_test_estimated_C.csv')\n",
    "\n",
    "predictions_C = predictor.predict(submission_data)\n",
    "\n",
    "\n",
    "create_submission(predictions_A,predictions_B, predictions_C)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
