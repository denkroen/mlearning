{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location A\n",
      "The df1 and df2 data frames have different columns.\n",
      "Columns unique to df1: set()\n",
      "Columns unique to df2: {'date_calc'}\n",
      "The df1 and df3 data frames have different columns.\n",
      "Columns unique to df1: set()\n",
      "Columns unique to df3: {'date_calc'}\n",
      "Both df2 and df3 data frames have the same columns.\n",
      "location B\n",
      "The df1 and df2 data frames have different columns.\n",
      "Columns unique to df1: set()\n",
      "Columns unique to df2: {'date_calc'}\n",
      "The df1 and df3 data frames have different columns.\n",
      "Columns unique to df1: set()\n",
      "Columns unique to df3: {'date_calc'}\n",
      "Both df2 and df3 data frames have the same columns.\n",
      "location C\n",
      "The df1 and df2 data frames have different columns.\n",
      "Columns unique to df1: set()\n",
      "Columns unique to df2: {'date_calc'}\n",
      "The df1 and df3 data frames have different columns.\n",
      "Columns unique to df1: set()\n",
      "Columns unique to df3: {'date_calc'}\n",
      "Both df2 and df3 data frames have the same columns.\n",
      "(29668, 31)\n",
      "(4418, 31)\n",
      "(1536, 31)\n",
      "(29233, 31)\n",
      "(4418, 31)\n",
      "(1536, 31)\n",
      "(29207, 31)\n",
      "(4418, 31)\n",
      "(1536, 31)\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "# from Preprocessing import *\n",
    "\n",
    "''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "Preprocessing functions\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "def drop_consecutive_nonzero_repeats(df):\n",
    "    count = 0\n",
    "    previous_value = None\n",
    "    indices_to_drop = []\n",
    "\n",
    "    for i, value in enumerate(df['pv_measurement']):\n",
    "        if value != 0:  # Exclude zeros\n",
    "            if value == previous_value:\n",
    "                count += 1\n",
    "                if count > 1:  # 3 consecutive times the same number\n",
    "                    # Mark the indices to be dropped\n",
    "                    indices_to_drop.extend(list(range(i - count, i + 1)))\n",
    "            else:\n",
    "                count = 0\n",
    "            previous_value = value\n",
    "\n",
    "    # Drop the rows with consecutive repeats\n",
    "    df = df.drop(indices_to_drop)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# drop consecutive zeros\n",
    "def drop_consecutive_zero_repeats(df):\n",
    "    count = 0\n",
    "    previous_value = None\n",
    "    indices_to_drop = []\n",
    "\n",
    "    for i, value in enumerate(df['pv_measurement']):\n",
    "        if value == previous_value:\n",
    "            count += 1\n",
    "            if count > 22:  # 24 consecutive times zero\n",
    "                # Mark the indices to be dropped\n",
    "                indices_to_drop.extend(list(range(i - count, i + 1)))\n",
    "        else:\n",
    "            count = 0\n",
    "        previous_value = value\n",
    "\n",
    "    # Drop the rows with consecutive repeats\n",
    "    df.drop(indices_to_drop, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocessing(df,target,soort_data):\n",
    "    \n",
    "        target.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "        \n",
    "        # 1. Attempt to drop the 'date_calc' column (if it exists) in each DataFrame\n",
    "        if 'date_calc' in df.columns:\n",
    "            df = df.drop('date_calc', axis=1)\n",
    "\n",
    "        imputer = IterativeImputer()\n",
    "\n",
    "        # Select the numeric columns for imputation\n",
    "        columns_to_impute = ['ceiling_height_agl:m']\n",
    "\n",
    "        # Perform imputation\n",
    "        df[columns_to_impute] = imputer.fit_transform(df[columns_to_impute])\n",
    "\n",
    "\n",
    "        \n",
    "        df = df.drop(\"snow_density:kgm3\", axis=1)\n",
    "        df = df.drop(\"elevation:m\", axis=1)\n",
    "        df = df.drop(\"wind_speed_w_1000hPa:ms\", axis=1)\n",
    "        df = df.drop(\"super_cooled_liquid_water:kgm2\", axis=1)\n",
    "        df = df.drop(\"rain_water:kgm2\", axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        filter1 = [col for col in df.columns if \"snow\" not in col]\n",
    "        df = df[filter1]\n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "       # 2. Linear interpolation for all columns\n",
    "        #for features in df.columns:\n",
    "        #    if features == 'snow_density:kgm3':\n",
    "        #        df[features].fillna(0, inplace=True)\n",
    "        #    else:\n",
    "                # Interpolate missing values using linear interpolation\n",
    "        #        df[features] = df[features].interpolate(method='linear')\n",
    "        #print(df.shape)\n",
    "        # # 3. Remove columns with constant data\n",
    "        # unique_counts = df.nunique()\n",
    "        # constant_features = unique_counts[unique_counts == 1].index\n",
    "        # df = df.drop(columns=constant_features, axis=1)\n",
    "\n",
    "        # 4. Set 'date_forecast' as the index and resample to hourly data\n",
    "        df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "        df.set_index('date_forecast', inplace=True)\n",
    "        df = df.resample('H').mean()\n",
    "        print(df.shape)\n",
    "\n",
    "        # 5. Merge the first and second DataFrames in the preprocessed_features list \n",
    "        # 6. Merge the first and third DataFrames in the input_features list\n",
    "\n",
    "        if soort_data == 'train_observed' or soort_data == 'train_estimated':\n",
    "            df = pd.merge(df, target, on='date_forecast', how='inner')\n",
    "\n",
    "        # 7. Conditional operations for 'pv_measurement' column (if it exists).\n",
    "        if 'pv_measurement' in df.columns:\n",
    "            # Define and use functions to drop consecutive non-zero and zero repeats (not provided)\n",
    "            #df = drop_consecutive_nonzero_repeats(df)\n",
    "            df = drop_consecutive_zero_repeats(df)\n",
    "            #df = df.fillna(0)\n",
    "            \n",
    "         # 8. Attempt to drop the 'date_forecast' column (if it exists) in each DataFrame\n",
    "        if 'date_forecast' in df.columns:\n",
    "            df = df.drop('date_forecast', axis=1)\n",
    "            \n",
    "        # Drop all rows where all columns are empty\n",
    "        if soort_data == 'train_observed' or soort_data == 'train_estimated':\n",
    "            df = df.dropna(how='any')\n",
    "        else:\n",
    "            df = df.dropna(how='all')\n",
    "            #df = df.fillna(0)\n",
    "\n",
    "        return df\n",
    "\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "def check_dataframe_same_columns(df1, df2, df3):\n",
    "    # Check and print column comparisons for df1 and df2\n",
    "    if df1.columns.equals(df2.columns):\n",
    "        print(\"Both df1 and df2 data frames have the same columns.\")\n",
    "    else:\n",
    "        print(\"The df1 and df2 data frames have different columns.\")\n",
    "        unique_columns_df1 = set(df1.columns) - set(df2.columns)\n",
    "        unique_columns_df2 = set(df2.columns) - set(df1.columns)\n",
    "        print(\"Columns unique to df1:\", unique_columns_df1)\n",
    "        print(\"Columns unique to df2:\", unique_columns_df2)\n",
    "\n",
    "    # Check and print column comparisons for df1 and df3\n",
    "    if df1.columns.equals(df3.columns):\n",
    "        print(\"Both df1 and df3 data frames have the same columns.\")\n",
    "    else:\n",
    "        print(\"The df1 and df3 data frames have different columns.\")\n",
    "        unique_columns_df1 = set(df1.columns) - set(df3.columns)\n",
    "        unique_columns_df3 = set(df3.columns) - set(df1.columns)\n",
    "        print(\"Columns unique to df1:\", unique_columns_df1)\n",
    "        print(\"Columns unique to df3:\", unique_columns_df3)\n",
    "\n",
    "    # Check and print column comparisons for df2 and df3\n",
    "    if df2.columns.equals(df3.columns):\n",
    "        print(\"Both df2 and df3 data frames have the same columns.\")\n",
    "    else:\n",
    "        print(\"The df2 and df3 data frames have different columns.\")\n",
    "        unique_columns_df2 = set(df2.columns) - set(df3.columns)\n",
    "        unique_columns_df3 = set(df3.columns) - set(df2.columns)\n",
    "        print(\"Columns unique to df2:\", unique_columns_df2)\n",
    "        print(\"Columns unique to df3:\", unique_columns_df3)\n",
    "\n",
    "\n",
    "\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "Functions\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "#define function to load all the data\n",
    "def load_data(location):\n",
    "    target = pd.read_parquet(f'{location}/raw/train_targets.parquet')\n",
    "    train_observed = pd.read_parquet(f'{location}/raw/X_train_observed.parquet')\n",
    "    train_estimated = pd.read_parquet(f'{location}/raw/X_train_estimated.parquet')\n",
    "    test_estimated = pd.read_parquet(f'{location}/raw/X_test_estimated.parquet')\n",
    "    \n",
    "    #put all the data of one location into a list\n",
    "    data = [target, train_observed, train_estimated, test_estimated]\n",
    "    return data\n",
    "\n",
    "\n",
    "#preprocess the three different datasets for all locations\n",
    "def preprocess_data(data):\n",
    "    train_observed = preprocessing(data[1],data[0],'train_observed')\n",
    "    train_estimated = preprocessing(data[2],data[0],'train_estimated')\n",
    "    test_estimated = preprocessing(data[3],data[0],'test_estimated')\n",
    "    data = [train_observed, train_estimated, test_estimated]\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_to_file(data_to_file,location):\n",
    "    #saving train estimated data to csv\n",
    "    data_to_file[0].to_csv(f'{location}/preproc_train_observed_{location}.csv', index=False)\n",
    "    data_to_file[1].to_csv(f'{location}/preproc_train_estimated_{location}.csv', index=False)\n",
    "    data_to_file[2].to_csv(f'{location}/preproc_test_estimated_{location}.csv', index=False)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "Main pipline\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "\n",
    "\n",
    "#load all the data and put them in a separate list for every location\n",
    "data_A = load_data('A')\n",
    "data_B = load_data('B')\n",
    "data_C = load_data('C')\n",
    "\n",
    "#check if columns are the same\n",
    "print('location A')\n",
    "check_dataframe_same_columns(data_A[1],data_A[2],data_A[3])\n",
    "print('location B')\n",
    "check_dataframe_same_columns(data_B[1],data_B[2],data_B[3])\n",
    "print('location C')\n",
    "check_dataframe_same_columns(data_C[1],data_C[2],data_C[3])\n",
    "\n",
    "preprocessed_A = preprocess_data(data_A)\n",
    "preprocessed_B = preprocess_data(data_B)\n",
    "preprocessed_C = preprocess_data(data_C)\n",
    "\n",
    "#save preprocessed data\n",
    "preprocessed_A = save_to_file(preprocessed_A,'A')\n",
    "preprocessed_B = save_to_file(preprocessed_B,'B')\n",
    "preprocessed_C = save_to_file(preprocessed_C,'C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "Functions\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "def delete_snow(df):\n",
    "    filter = [col for col in df.columns if \"snow\" not in col]\n",
    "    df = df[filter]\n",
    "    return df\n",
    "\n",
    "def create_wind(df):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    df[\"windSpeed\"] = np.sqrt(df[\"wind_speed_u_10m:ms\"]**2 + df[\"wind_speed_v_10m:ms\"]**2)\n",
    "    df[\"windAngle\"] = np.arctan2(df[\"wind_speed_v_10m:ms\"], df[\"wind_speed_u_10m:ms\"])\n",
    "    return df\n",
    "\n",
    "def create_time_feature(df):\n",
    "    df['hourofday'] = df['date_forecast'].dt.hour\n",
    "    df['dayofmonth'] = df['date_forecast'].dt.day\n",
    "    df['month'] = df['date_forecast'].dt.month\n",
    "    df['year'] = df['date_forecast'].dt.year\n",
    "    df['dayofweek'] = df['date_forecast'].dt.dayofweek\n",
    "    df['dayofyear'] = df['date_forecast'].dt.dayofyear\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_submission(predictions_A,predictions_B,predictions_C):\n",
    "\n",
    "    combined_predictions = pd.concat([predictions_A,predictions_B,predictions_C], axis=0)\n",
    "    combined_predictions = combined_predictions.reset_index(drop=True)\n",
    "    combined_predictions.index.name = 'id'\n",
    "    combined_predictions.rename('prediction', inplace=True)\n",
    "    # combined_predictions = combined_predictions.drop('Unnamed: 0', axis=1)\n",
    "    combined_predictions.to_csv('auto_predictions_finalv31_bare_50_80.csv')\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "train_data = TabularDataset('A/preproc_train_observed_A.csv')\n",
    "train_data.head()\n",
    "label = 'pv_measurement'\n",
    "test_data = TabularDataset(f'A/preproc_train_estimated_A.csv')\n",
    "\n",
    "#transform date\n",
    "train_data['date_forecast'] = pd.to_datetime(train_data.date_forecast, format='%Y-%m-%d %H:%M:%S')\n",
    "test_data['date_forecast'] = pd.to_datetime(test_data.date_forecast, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#create/delete features\n",
    "train_data=create_time_feature(train_data)\n",
    "test_data=create_time_feature(test_data)\n",
    "\n",
    "train_data = delete_snow(train_data)\n",
    "test_data = delete_snow(test_data)\n",
    "\n",
    "train_data = create_wind(train_data)\n",
    "test_data = create_wind(test_data)\n",
    "\n",
    "\n",
    "#Add validation data to training\n",
    "percentage = 0.50\n",
    "num_rows = int(len(test_data) * percentage)\n",
    "sample = test_data.sample(n=num_rows, random_state=42)\n",
    "test_data = test_data.drop(sample.index)\n",
    "train_data = pd.concat([train_data,sample]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#Train and predict\n",
    "predictor = TabularPredictor(label=label, eval_metric='mean_absolute_error').fit(train_data, tuning_data=test_data)\n",
    "\n",
    "#only for analysis. Takes some time to compute.\n",
    "#x = predictor.feature_importance(test_data)\n",
    "#print(x)\n",
    "\n",
    "\n",
    "submission_data = TabularDataset('A/preproc_test_estimated_A.csv')\n",
    "submission_data['date_forecast'] = pd.to_datetime(submission_data.date_forecast, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "submission_data=create_time_feature(submission_data)\n",
    "submission_data = create_wind(submission_data)\n",
    "submission_data = delete_snow(submission_data)\n",
    "\n",
    "\n",
    "\n",
    "predictions_A = predictor.predict(submission_data)\n",
    "\n",
    "##########B\n",
    "\n",
    "train_data = TabularDataset('B/preproc_train_observed_B.csv')\n",
    "label = 'pv_measurement'\n",
    "test_data = TabularDataset(f'B/preproc_train_estimated_B.csv')\n",
    "\n",
    "#Add validation data to training\n",
    "percentage = 0.50\n",
    "num_rows = int(len(test_data) * percentage)\n",
    "sample = test_data.sample(n=num_rows, random_state=42)\n",
    "test_data = test_data.drop(sample.index)\n",
    "train_data = pd.concat([train_data,sample]).reset_index(drop=True)\n",
    "\n",
    "predictor = TabularPredictor(label=label, eval_metric='mean_absolute_error').fit(train_data, presets='best_quality', ag_args_fit={'num_gpus':1}, num_stack_levels=0,tuning_data=test_data,use_bag_holdout=True)\n",
    "x = predictor.feature_importance(test_data)\n",
    "\n",
    "print(x)\n",
    "\n",
    "submission_data = TabularDataset('B/preproc_test_estimated_B.csv')\n",
    "\n",
    "predictions_B = predictor.predict(submission_data)\n",
    "\n",
    "###########C\n",
    "\n",
    "train_data = TabularDataset('C/preproc_train_observed_C.csv')\n",
    "label = 'pv_measurement'\n",
    "test_data = TabularDataset(f'C/preproc_train_estimated_C.csv')\n",
    "#train_data = pd.concat([train_data,test_data],ignore_index=True)\n",
    "\n",
    "percentage = 0.50\n",
    "num_rows = int(len(test_data) * percentage)\n",
    "sample = test_data.sample(n=num_rows, random_state=42)\n",
    "test_data = test_data.drop(sample.index)\n",
    "train_data = pd.concat([train_data,sample]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "predictor = TabularPredictor(label=label, eval_metric='mean_absolute_error').fit(train_data, presets='best_quality', ag_args_fit={'num_gpus':1}, num_stack_levels=0,tuning_data=test_data,use_bag_holdout=True)\n",
    "x = predictor.feature_importance(test_data)\n",
    "\n",
    "print(x)\n",
    "\n",
    "submission_data = TabularDataset('C/preproc_test_estimated_C.csv')\n",
    "\n",
    "predictions_C = predictor.predict(submission_data)\n",
    "\n",
    "\n",
    "create_submission(predictions_A,predictions_B, predictions_C)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
