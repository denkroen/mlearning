{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature exploring\n",
    "\n",
    "In the first steps we explored which features are provided by the dataset and how they interact with eachother. One important feature group for us were the wind features. We found out, that wind is cooling down solar panels during the day and thus increasing their efficiency. Furthermore we found some articles that elaborate on the impact of the wind direction on the performance on solar panels and that it can be derived from the measured u and v components. So we decided to create the wind direction feature and also to estimate the wind speed with the u and v component. The calculated windspeed did not match up with the provided measured windspeed (possibly because of measurement inaccuracys) but it still increased the performance of our model.\n",
    "\n",
    "We also explored a group of sun features (sun_azimuth and sun_elevation). With these features and information about the tilt angle and location of the solar panel, it is possible to derive information about the energy capture. Since we dont have information about the position or the tilt of the solar panel, we scraped the idea in the beginning.\n",
    "\n",
    "During testing of our model, we also found out, that the features related to snow where harmful for the predictions. Therefore we removed them.\n",
    "We also decided to remove constant features like the \"wind speed w\" and the \"elevation\", since they dont give us any relevant information.\n",
    "\n",
    "Furthermore we looked into different individual features that were not intuitive for us. We researched on how they impact the solar energy production in the documentation and the internet:\n",
    "\n",
    "Supercooled Liquid Water refers to liquid water droplets below the freezing point and can reduce the transmission of sunlight. It can be used to predict icing events.\n",
    "\n",
    "visibility gives information about fog. If the visibility is below 1 km, the weather is declared as foggy. This is the reason we scaled it to kilometers for our model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'visibility'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'visibility'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/dennis/code/ntnu/mlearning/1.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/dennis/code/ntnu/mlearning/1.ipynb#W1sZmlsZQ%3D%3D?line=166'>167</a>\u001b[0m data_B \u001b[39m=\u001b[39m load_data(\u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/dennis/code/ntnu/mlearning/1.ipynb#W1sZmlsZQ%3D%3D?line=167'>168</a>\u001b[0m data_C \u001b[39m=\u001b[39m load_data(\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/dennis/code/ntnu/mlearning/1.ipynb#W1sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m preprocessed_A \u001b[39m=\u001b[39m preprocess_data(data_A,\u001b[39m\"\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/dennis/code/ntnu/mlearning/1.ipynb#W1sZmlsZQ%3D%3D?line=170'>171</a>\u001b[0m preprocessed_B \u001b[39m=\u001b[39m preprocess_data(data_B,\u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/dennis/code/ntnu/mlearning/1.ipynb#W1sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m preprocessed_C \u001b[39m=\u001b[39m preprocess_data(data_C,\u001b[39m\"\u001b[39m\u001b[39mc\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/dennis/code/ntnu/mlearning/1.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/dennis/code/ntnu/mlearning/1.ipynb#W1sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_data\u001b[39m(data,\u001b[39mid\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/dennis/code/ntnu/mlearning/1.ipynb#W1sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m     train_observed \u001b[39m=\u001b[39m preprocessing(data[\u001b[39m1\u001b[39;49m],data[\u001b[39m0\u001b[39;49m],\u001b[39m'\u001b[39;49m\u001b[39mtrain_observed\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39mid\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/dennis/code/ntnu/mlearning/1.ipynb#W1sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m     train_estimated \u001b[39m=\u001b[39m preprocessing(data[\u001b[39m2\u001b[39m],data[\u001b[39m0\u001b[39m],\u001b[39m'\u001b[39m\u001b[39mtrain_estimated\u001b[39m\u001b[39m'\u001b[39m,\u001b[39mid\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/dennis/code/ntnu/mlearning/1.ipynb#W1sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m     test_estimated \u001b[39m=\u001b[39m preprocessing(data[\u001b[39m3\u001b[39m],data[\u001b[39m0\u001b[39m],\u001b[39m'\u001b[39m\u001b[39mtest_estimated\u001b[39m\u001b[39m'\u001b[39m,\u001b[39mid\u001b[39m)\n",
      "\u001b[1;32m/home/dennis/code/ntnu/mlearning/1.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dennis/code/ntnu/mlearning/1.ipynb#W1sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop(\u001b[39m\"\u001b[39m\u001b[39melevation:m\u001b[39m\u001b[39m\"\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dennis/code/ntnu/mlearning/1.ipynb#W1sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop(\u001b[39m\"\u001b[39m\u001b[39mwind_speed_w_1000hPa:ms\u001b[39m\u001b[39m\"\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dennis/code/ntnu/mlearning/1.ipynb#W1sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     df[\u001b[39m\"\u001b[39m\u001b[39mvisibility\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39;49m\u001b[39mvisibility\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dennis/code/ntnu/mlearning/1.ipynb#W1sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39m#df = df.drop(\"super_cooled_liquid_water:kgm2\", axis=1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dennis/code/ntnu/mlearning/1.ipynb#W1sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     \u001b[39m#df = df.drop(\"rain_water:kgm2\", axis=1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dennis/code/ntnu/mlearning/1.ipynb#W1sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dennis/code/ntnu/mlearning/1.ipynb#W1sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m         \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dennis/code/ntnu/mlearning/1.ipynb#W1sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m# Calculate the percentage of NaN values in each column\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dennis/code/ntnu/mlearning/1.ipynb#W1sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m nan_percentage \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39misna()\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'visibility'"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "# from Preprocessing import *\n",
    "\n",
    "''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "Preprocessing functions\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "def drop_consecutive_nonzero_repeats(df):\n",
    "    count = 0\n",
    "    previous_value = None\n",
    "    indices_to_drop = []\n",
    "\n",
    "    for i, value in enumerate(df['pv_measurement']):\n",
    "        if value != 0:  # Exclude zeros\n",
    "            if value == previous_value:\n",
    "                count += 1\n",
    "                if count > 1:  # 3 consecutive times the same number\n",
    "                    # Mark the indices to be dropped\n",
    "                    indices_to_drop.extend(list(range(i - count, i + 1)))\n",
    "            else:\n",
    "                count = 0\n",
    "            previous_value = value\n",
    "\n",
    "    # Drop the rows with consecutive repeats\n",
    "    df = df.drop(indices_to_drop)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocessing(df,target,soort_data,id):\n",
    "    \n",
    "        target.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "        \n",
    "        # 1. Attempt to drop the 'date_calc' column (if it exists) in each DataFrame\n",
    "        if 'date_calc' in df.columns:\n",
    "            df = df.drop('date_calc', axis=1)\n",
    "\n",
    "        imputer = IterativeImputer()\n",
    "\n",
    "        # Select the numeric columns for imputation\n",
    "        columns_to_impute = ['ceiling_height_agl:m','cloud_base_agl:m']\n",
    "\n",
    "        # Perform imputation\n",
    "        df[columns_to_impute] = imputer.fit_transform(df[columns_to_impute])\n",
    "\n",
    "        #df['cloud_base_agl:m'].interpolate(method='linear', inplace=True)#.fillna(method='ffill', inplace=True)\n",
    "\n",
    "\n",
    "        if id == 'a':\n",
    "            df = df.drop(\"snow_density:kgm3\", axis=1)\n",
    "            df = df.drop(\"elevation:m\", axis=1)\n",
    "            df = df.drop(\"wind_speed_w_1000hPa:ms\", axis=1)\n",
    "            df[\"visibility\"] = df[\"visibility\"] * 1000\n",
    "            #df = df.drop(\"super_cooled_liquid_water:kgm2\", axis=1)\n",
    "            #df = df.drop(\"rain_water:kgm2\", axis=1)\n",
    "\n",
    "            filter1 = [col for col in df.columns if \"snow\" not in col]\n",
    "            df = df[filter1]\n",
    "\n",
    "                \n",
    "        # Calculate the percentage of NaN values in each column\n",
    "        nan_percentage = df.isna().mean()\n",
    "\n",
    "        # Print columns with more than 50% of values as NaN\n",
    "        columns_with_more_than_50_percent_nan = nan_percentage[nan_percentage > 0.1].index\n",
    "        print(columns_with_more_than_50_percent_nan)\n",
    "\n",
    "\n",
    "       # 2. Linear interpolation for all columns\n",
    "        #for features in df.columns:\n",
    "        #    if features == 'snow_density:kgm3':\n",
    "        #        df[features].fillna(0, inplace=True)\n",
    "        #    else:\n",
    "                # Interpolate missing values using linear interpolation\n",
    "        #        df[features] = df[features].interpolate(method='linear')\n",
    "        #print(df.shape)\n",
    "        # # 3. Remove columns with constant data\n",
    "        unique_counts = df.nunique()\n",
    "        constant_features = unique_counts[unique_counts == 1].index\n",
    "        df = df.drop(columns=constant_features, axis=1)\n",
    "\n",
    "        # 4. Set 'date_forecast' as the index and resample to hourly data\n",
    "        df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "        df.set_index('date_forecast', inplace=True)\n",
    "        df = df.resample('H').mean()\n",
    "        print(df.shape)\n",
    "\n",
    "        # 5. Merge the first and second DataFrames in the preprocessed_features list \n",
    "        # 6. Merge the first and third DataFrames in the input_features list\n",
    "\n",
    "        if soort_data == 'train_observed' or soort_data == 'train_estimated':\n",
    "            df = pd.merge(df, target, on='date_forecast', how='inner')\n",
    "\n",
    "        # 7. Conditional operations for 'pv_measurement' column (if it exists).\n",
    "        if 'pv_measurement' in df.columns:\n",
    "            # Define and use functions to drop consecutive non-zero and zero repeats (not provided)\n",
    "            df = drop_consecutive_nonzero_repeats(df)\n",
    "            #df = drop_consecutive_zero_repeats(df)\n",
    "            #df = df.fillna(0)\n",
    "            \n",
    "         # 8. Attempt to drop the 'date_forecast' column (if it exists) in each DataFrame\n",
    "        if 'date_forecast' in df.columns:\n",
    "            df = df.drop('date_forecast', axis=1)\n",
    "            \n",
    "        # Drop all rows where all columns are empty\n",
    "        if soort_data == 'train_observed' or soort_data == 'train_estimated':\n",
    "            if id == \"a\":\n",
    "                df = df.fillna(0)#dropna(how='any')\n",
    "            else:\n",
    "                df = df.fillna(0)\n",
    "        else:\n",
    "            df = df.dropna(how='all')\n",
    "            #df = df.fillna(0)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "Functions\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "#define function to load all the data\n",
    "def load_data(location):\n",
    "    target = pd.read_parquet(f'{location}/raw/train_targets.parquet')\n",
    "    train_observed = pd.read_parquet(f'{location}/raw/X_train_observed.parquet')\n",
    "    train_estimated = pd.read_parquet(f'{location}/raw/X_train_estimated.parquet')\n",
    "    test_estimated = pd.read_parquet(f'{location}/raw/X_test_estimated.parquet')\n",
    "    \n",
    "    #put all the data of one location into a list\n",
    "    data = [target, train_observed, train_estimated, test_estimated]\n",
    "    return data\n",
    "\n",
    "\n",
    "#preprocess the three different datasets for all locations\n",
    "def preprocess_data(data,id):\n",
    "    train_observed = preprocessing(data[1],data[0],'train_observed',id)\n",
    "    train_estimated = preprocessing(data[2],data[0],'train_estimated',id)\n",
    "    test_estimated = preprocessing(data[3],data[0],'test_estimated',id)\n",
    "    data = [train_observed, train_estimated, test_estimated]\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_to_file(data_to_file,location):\n",
    "    #saving train estimated data to csv\n",
    "    data_to_file[0].to_csv(f'{location}/preproc_train_observed_{location}.csv', index=False)\n",
    "    data_to_file[1].to_csv(f'{location}/preproc_train_estimated_{location}.csv', index=False)\n",
    "    data_to_file[2].to_csv(f'{location}/preproc_test_estimated_{location}.csv', index=False)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "Main pipline\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "\n",
    "\n",
    "#load all the data and put them in a separate list for every location\n",
    "data_A = load_data('A')\n",
    "data_B = load_data('B')\n",
    "data_C = load_data('C')\n",
    "\n",
    "preprocessed_A = preprocess_data(data_A,\"a\")\n",
    "preprocessed_B = preprocess_data(data_B,\"b\")\n",
    "preprocessed_C = preprocess_data(data_C,\"c\")\n",
    "\n",
    "#save preprocessed data\n",
    "preprocessed_A = save_to_file(preprocessed_A,'A')\n",
    "preprocessed_B = save_to_file(preprocessed_B,'B')\n",
    "preprocessed_C = save_to_file(preprocessed_C,'C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "Functions\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "def delete_snow(df):\n",
    "    filter = [col for col in df.columns if \"snow\" not in col]\n",
    "    df = df[filter]\n",
    "    return df\n",
    "\n",
    "def create_wind(df):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    df[\"windSpeed\"] = np.sqrt(df[\"wind_speed_u_10m:ms\"]**2 + df[\"wind_speed_v_10m:ms\"]**2)\n",
    "    df[\"windAngle\"] = np.arctan2(df[\"wind_speed_v_10m:ms\"], df[\"wind_speed_u_10m:ms\"])\n",
    "    return df\n",
    "\n",
    "def create_time_feature(df):\n",
    "    df['hourofday'] = df['date_forecast'].dt.hour\n",
    "    df['dayofmonth'] = df['date_forecast'].dt.day\n",
    "    df['month'] = df['date_forecast'].dt.month\n",
    "    df['year'] = df['date_forecast'].dt.year\n",
    "    df['dayofweek'] = df['date_forecast'].dt.dayofweek\n",
    "    df['dayofyear'] = df['date_forecast'].dt.dayofyear\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_submission(predictions_A,predictions_B,predictions_C):\n",
    "\n",
    "    combined_predictions = pd.concat([predictions_A,predictions_B,predictions_C], axis=0)\n",
    "    combined_predictions = combined_predictions.reset_index(drop=True)\n",
    "    combined_predictions.index.name = 'id'\n",
    "    combined_predictions.rename('prediction', inplace=True)\n",
    "    # combined_predictions = combined_predictions.drop('Unnamed: 0', axis=1)\n",
    "    combined_predictions.to_csv('auto_predictions_finalv31_bare_50_80.csv')\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "train_data = TabularDataset('A/preproc_train_observed_A.csv')\n",
    "train_data.head()\n",
    "label = 'pv_measurement'\n",
    "test_data = TabularDataset(f'A/preproc_train_estimated_A.csv')\n",
    "\n",
    "#transform date\n",
    "train_data['date_forecast'] = pd.to_datetime(train_data.date_forecast, format='%Y-%m-%d %H:%M:%S')\n",
    "test_data['date_forecast'] = pd.to_datetime(test_data.date_forecast, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#create/delete features\n",
    "train_data=create_time_feature(train_data)\n",
    "test_data=create_time_feature(test_data)\n",
    "\n",
    "train_data = delete_snow(train_data)\n",
    "test_data = delete_snow(test_data)\n",
    "\n",
    "train_data = create_wind(train_data)\n",
    "test_data = create_wind(test_data)\n",
    "\n",
    "\n",
    "#Add validation data to training\n",
    "percentage = 0.50\n",
    "num_rows = int(len(test_data) * percentage)\n",
    "sample = test_data.sample(n=num_rows, random_state=42)\n",
    "test_data = test_data.drop(sample.index)\n",
    "train_data = pd.concat([train_data,sample]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#Train and predict\n",
    "predictor = TabularPredictor(label=label, eval_metric='mean_absolute_error').fit(train_data, tuning_data=test_data)\n",
    "\n",
    "#only for analysis. Takes some time to compute.\n",
    "#x = predictor.feature_importance(test_data)\n",
    "#print(x)\n",
    "\n",
    "\n",
    "submission_data = TabularDataset('A/preproc_test_estimated_A.csv')\n",
    "submission_data['date_forecast'] = pd.to_datetime(submission_data.date_forecast, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "submission_data=create_time_feature(submission_data)\n",
    "submission_data = create_wind(submission_data)\n",
    "submission_data = delete_snow(submission_data)\n",
    "\n",
    "\n",
    "\n",
    "predictions_A = predictor.predict(submission_data)\n",
    "\n",
    "##########B\n",
    "\n",
    "train_data = TabularDataset('B/preproc_train_observed_B.csv')\n",
    "label = 'pv_measurement'\n",
    "test_data = TabularDataset(f'B/preproc_train_estimated_B.csv')\n",
    "\n",
    "#Add validation data to training\n",
    "percentage = 0.50\n",
    "num_rows = int(len(test_data) * percentage)\n",
    "sample = test_data.sample(n=num_rows, random_state=42)\n",
    "test_data = test_data.drop(sample.index)\n",
    "train_data = pd.concat([train_data,sample]).reset_index(drop=True)\n",
    "\n",
    "predictor = TabularPredictor(label=label, eval_metric='mean_absolute_error').fit(train_data, presets='best_quality', ag_args_fit={'num_gpus':1}, num_stack_levels=0,tuning_data=test_data,use_bag_holdout=True)\n",
    "x = predictor.feature_importance(test_data)\n",
    "\n",
    "print(x)\n",
    "\n",
    "submission_data = TabularDataset('B/preproc_test_estimated_B.csv')\n",
    "\n",
    "predictions_B = predictor.predict(submission_data)\n",
    "\n",
    "###########C\n",
    "\n",
    "train_data = TabularDataset('C/preproc_train_observed_C.csv')\n",
    "label = 'pv_measurement'\n",
    "test_data = TabularDataset(f'C/preproc_train_estimated_C.csv')\n",
    "#train_data = pd.concat([train_data,test_data],ignore_index=True)\n",
    "\n",
    "percentage = 0.50\n",
    "num_rows = int(len(test_data) * percentage)\n",
    "sample = test_data.sample(n=num_rows, random_state=42)\n",
    "test_data = test_data.drop(sample.index)\n",
    "train_data = pd.concat([train_data,sample]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "predictor = TabularPredictor(label=label, eval_metric='mean_absolute_error').fit(train_data, presets='best_quality', ag_args_fit={'num_gpus':1}, num_stack_levels=0,tuning_data=test_data,use_bag_holdout=True)\n",
    "x = predictor.feature_importance(test_data)\n",
    "\n",
    "print(x)\n",
    "\n",
    "submission_data = TabularDataset('C/preproc_test_estimated_C.csv')\n",
    "\n",
    "predictions_C = predictor.predict(submission_data)\n",
    "\n",
    "\n",
    "create_submission(predictions_A,predictions_B, predictions_C)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
