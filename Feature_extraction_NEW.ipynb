{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T12:08:30.471858Z",
     "start_time": "2023-11-08T12:08:24.723126800Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location A\n",
      "The df1 and df2 data frames have different columns.\n",
      "Columns unique to df1: set()\n",
      "Columns unique to df2: {'date_calc'}\n",
      "The df1 and df3 data frames have different columns.\n",
      "Columns unique to df1: set()\n",
      "Columns unique to df3: {'date_calc'}\n",
      "Both df2 and df3 data frames have the same columns.\n",
      "location B\n",
      "The df1 and df2 data frames have different columns.\n",
      "Columns unique to df1: set()\n",
      "Columns unique to df2: {'date_calc'}\n",
      "The df1 and df3 data frames have different columns.\n",
      "Columns unique to df1: set()\n",
      "Columns unique to df3: {'date_calc'}\n",
      "Both df2 and df3 data frames have the same columns.\n",
      "location C\n",
      "The df1 and df2 data frames have different columns.\n",
      "Columns unique to df1: set()\n",
      "Columns unique to df2: {'date_calc'}\n",
      "The df1 and df3 data frames have different columns.\n",
      "Columns unique to df1: set()\n",
      "Columns unique to df3: {'date_calc'}\n",
      "Both df2 and df3 data frames have the same columns.\n",
      "(118669, 46)\n",
      "(29668, 45)\n",
      "(17576, 46)\n",
      "(4418, 45)\n",
      "(2880, 46)\n",
      "(1536, 45)\n",
      "(116929, 46)\n",
      "(29233, 45)\n",
      "(17576, 46)\n",
      "(4418, 45)\n",
      "(2880, 46)\n",
      "(1536, 45)\n",
      "(116825, 46)\n",
      "(29207, 45)\n",
      "(17576, 46)\n",
      "(4418, 45)\n",
      "(2880, 46)\n",
      "(1536, 45)\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "# from Preprocessing import *\n",
    "\n",
    "''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "Preprocessing functions\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "def drop_consecutive_nonzero_repeats(df):\n",
    "    count = 0\n",
    "    previous_value = None\n",
    "    indices_to_drop = []\n",
    "\n",
    "    for i, value in enumerate(df['pv_measurement']):\n",
    "        if value != 0:  # Exclude zeros\n",
    "            if value == previous_value:\n",
    "                count += 1\n",
    "                if count > 1:  # 3 consecutive times the same number\n",
    "                    # Mark the indices to be dropped\n",
    "                    indices_to_drop.extend(list(range(i - count, i + 1)))\n",
    "            else:\n",
    "                count = 0\n",
    "            previous_value = value\n",
    "\n",
    "    # Drop the rows with consecutive repeats\n",
    "    df = df.drop(indices_to_drop)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# drop consecutive zeros\n",
    "def drop_consecutive_zero_repeats(df):\n",
    "    count = 0\n",
    "    previous_value = None\n",
    "    indices_to_drop = []\n",
    "\n",
    "    for i, value in enumerate(df['pv_measurement']):\n",
    "        if value == previous_value:\n",
    "            count += 1\n",
    "            if count > 22:  # 24 consecutive times zero\n",
    "                # Mark the indices to be dropped\n",
    "                indices_to_drop.extend(list(range(i - count, i + 1)))\n",
    "        else:\n",
    "            count = 0\n",
    "        previous_value = value\n",
    "\n",
    "    # Drop the rows with consecutive repeats\n",
    "    df.drop(indices_to_drop, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocessing(df,target,soort_data):\n",
    "    \n",
    "        target.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "        \n",
    "        # 1. Attempt to drop the 'date_calc' column (if it exists) in each DataFrame\n",
    "        if 'date_calc' in df.columns:\n",
    "            df = df.drop('date_calc', axis=1)\n",
    "        \n",
    "        # 2. Linear interpolation for all columns\n",
    "        for features in df.columns:\n",
    "            if features == 'snow_density:kgm3':\n",
    "                df[features].fillna(0, inplace=True)\n",
    "            else:\n",
    "                # Interpolate missing values using linear interpolation\n",
    "                df[features] = df[features].interpolate(method='linear')\n",
    "        print(df.shape)\n",
    "        # # 3. Remove columns with constant data\n",
    "        # unique_counts = df.nunique()\n",
    "        # constant_features = unique_counts[unique_counts == 1].index\n",
    "        # df = df.drop(columns=constant_features, axis=1)\n",
    "\n",
    "        # 4. Set 'date_forecast' as the index and resample to hourly data\n",
    "        df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "        df.set_index('date_forecast', inplace=True)\n",
    "        df = df.resample('H').mean()\n",
    "        print(df.shape)\n",
    "\n",
    "        # 5. Merge the first and second DataFrames in the preprocessed_features list \n",
    "        # 6. Merge the first and third DataFrames in the input_features list\n",
    "\n",
    "        if soort_data == 'train_observed' or soort_data == 'train_estimated':\n",
    "            df = pd.merge(df, target, on='date_forecast', how='inner')\n",
    "\n",
    "        # 7. Conditional operations for 'pv_measurement' column (if it exists).\n",
    "        if 'pv_measurement' in df.columns:\n",
    "            # Define and use functions to drop consecutive non-zero and zero repeats (not provided)\n",
    "            #df = drop_consecutive_nonzero_repeats(df)\n",
    "            df = drop_consecutive_zero_repeats(df)\n",
    "            df = df.fillna(0)\n",
    "            \n",
    "         # 8. Attempt to drop the 'date_forecast' column (if it exists) in each DataFrame\n",
    "        if 'date_forecast' in df.columns:\n",
    "            df = df.drop('date_forecast', axis=1)\n",
    "            \n",
    "        # Drop all rows where all columns are empty\n",
    "        df = df.dropna(how='all')\n",
    "        return df\n",
    "\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "def check_dataframe_same_columns(df1, df2, df3):\n",
    "    # Check and print column comparisons for df1 and df2\n",
    "    if df1.columns.equals(df2.columns):\n",
    "        print(\"Both df1 and df2 data frames have the same columns.\")\n",
    "    else:\n",
    "        print(\"The df1 and df2 data frames have different columns.\")\n",
    "        unique_columns_df1 = set(df1.columns) - set(df2.columns)\n",
    "        unique_columns_df2 = set(df2.columns) - set(df1.columns)\n",
    "        print(\"Columns unique to df1:\", unique_columns_df1)\n",
    "        print(\"Columns unique to df2:\", unique_columns_df2)\n",
    "\n",
    "    # Check and print column comparisons for df1 and df3\n",
    "    if df1.columns.equals(df3.columns):\n",
    "        print(\"Both df1 and df3 data frames have the same columns.\")\n",
    "    else:\n",
    "        print(\"The df1 and df3 data frames have different columns.\")\n",
    "        unique_columns_df1 = set(df1.columns) - set(df3.columns)\n",
    "        unique_columns_df3 = set(df3.columns) - set(df1.columns)\n",
    "        print(\"Columns unique to df1:\", unique_columns_df1)\n",
    "        print(\"Columns unique to df3:\", unique_columns_df3)\n",
    "\n",
    "    # Check and print column comparisons for df2 and df3\n",
    "    if df2.columns.equals(df3.columns):\n",
    "        print(\"Both df2 and df3 data frames have the same columns.\")\n",
    "    else:\n",
    "        print(\"The df2 and df3 data frames have different columns.\")\n",
    "        unique_columns_df2 = set(df2.columns) - set(df3.columns)\n",
    "        unique_columns_df3 = set(df3.columns) - set(df2.columns)\n",
    "        print(\"Columns unique to df2:\", unique_columns_df2)\n",
    "        print(\"Columns unique to df3:\", unique_columns_df3)\n",
    "        \n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "Main pipline\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "#define function to load all the data\n",
    "def load_data(location):\n",
    "    target = pd.read_parquet(f'{location}/raw/train_targets.parquet')\n",
    "    train_observed = pd.read_parquet(f'{location}/raw/X_train_observed.parquet')\n",
    "    train_estimated = pd.read_parquet(f'{location}/raw/X_train_estimated.parquet')\n",
    "    test_estimated = pd.read_parquet(f'{location}/raw/X_test_estimated.parquet')\n",
    "    \n",
    "    #put all the data of one location into a list\n",
    "    data = [target, train_observed, train_estimated, test_estimated]\n",
    "    return data\n",
    "\n",
    "#load all the data and put them in a separate list for every location\n",
    "data_A = load_data('A')\n",
    "data_B = load_data('B')\n",
    "data_C = load_data('C')\n",
    "\n",
    "#check if columns are the same\n",
    "print('location A')\n",
    "check_dataframe_same_columns(data_A[1],data_A[2],data_A[3])\n",
    "print('location B')\n",
    "check_dataframe_same_columns(data_B[1],data_B[2],data_B[3])\n",
    "print('location C')\n",
    "check_dataframe_same_columns(data_C[1],data_C[2],data_C[3])\n",
    "\n",
    "#preprocess the three different datasets for all locations\n",
    "def preprocess_data(data):\n",
    "    train_observed = preprocessing(data[1],data[0],'train_observed')\n",
    "    train_estimated = preprocessing(data[2],data[0],'train_estimated')\n",
    "    test_estimated = preprocessing(data[3],data[0],'test_estimated')\n",
    "    data = [train_observed, train_estimated, test_estimated]\n",
    "    return data\n",
    "\n",
    "preprocessed_A = preprocess_data(data_A)\n",
    "preprocessed_B = preprocess_data(data_B)\n",
    "preprocessed_C = preprocess_data(data_C)\n",
    "\n",
    "# print('location A')\n",
    "# check_dataframe_same_columns(preprocessed_A[0],preprocessed_A[1],preprocessed_A[2])\n",
    "# print('location B')\n",
    "# check_dataframe_same_columns(preprocessed_B[0],preprocessed_B[1],preprocessed_B[2])\n",
    "# print('location C')\n",
    "# check_dataframe_same_columns(preprocessed_C[0],preprocessed_C[1],preprocessed_C[2])\n",
    "def save_to_file(data_to_file,location):\n",
    "    #saving train estimated data to csv\n",
    "    data_to_file[0].to_csv(f'{location}/preproc_train_observed_{location}.csv', index=False)\n",
    "    data_to_file[1].to_csv(f'{location}/preproc_train_estimated_{location}.csv', index=False)\n",
    "    data_to_file[2].to_csv(f'{location}/preproc_test_estimated_{location}.csv', index=False)\n",
    "    \n",
    "    #saving train estimated data to excel\n",
    "    # data_to_file[0].to_excel(f'{location}/preproc_train_observed_{location}.xlsx', index=False)\n",
    "    # data_to_file[1].to_excel(f'{location}/preproc_train_estimated_{location}.xlsx', index=False)\n",
    "    # data_to_file[2].to_excel(f'{location}/preproc_test_estimated_{location}.xlsx', index=False)\n",
    "\n",
    "#save preprocessed data\n",
    "preprocessed_A = save_to_file(preprocessed_A,'A')\n",
    "preprocessed_B = save_to_file(preprocessed_B,'B')\n",
    "preprocessed_C = save_to_file(preprocessed_C,'C')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def divide_dataset(train_observed):\n",
    "#     x_train = train_observed.drop('pv_measurement', axis = 1)\n",
    "#     y_train = train_observed['pv_measurement']\n",
    "#     training_data = [x_train, y_train]\n",
    "#     return training_data\n",
    "# \n",
    "# #retrieve the input features and target value separately for all locations\n",
    "# training_A = divide_dataset(preprocessed_A[0])\n",
    "# training_B = divide_dataset(preprocessed_B[0])\n",
    "# training_C = divide_dataset(preprocessed_C[0])\n",
    "# \n",
    "# \n",
    "# #define a function that applies a pearson correlation matrix using the target variable\n",
    "# def correlation_matrix(data):\n",
    "#     correlation = data[0].corr()\n",
    "# \n",
    "#     # # plot the entire correlation matrix\n",
    "#     # plt.figure(figsize=(64, 56))\n",
    "#     # sns.heatmap(correlation, annot=True, cmap=plt.cm.Reds)\n",
    "#     # plt.title(\"Correlation all features Heatmap\")\n",
    "#     # plt.show()\n",
    "# \n",
    "#     #Correlation with output variable\n",
    "#     cor_target = abs(correlation['pv_measurement'])\n",
    "#     # print('cor_target',cor_target)\n",
    "# \n",
    "#     #Selecting highly correlated features\n",
    "#     relevant_features = cor_target[cor_target>0.5] \n",
    "#     #print('relevant_features',relevant_features)\n",
    "#     relevant_features_label = list(cor_target[cor_target>0.5].index)\n",
    "#     #print('relevant_features_label',relevant_features_label)\n",
    "#     filtered_cor = correlation.loc[relevant_features_label, relevant_features_label]\n",
    "# \n",
    "#     # Using Pearson Correlation (compare with other input features)\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     sns.heatmap(filtered_cor, annot=True, cmap=plt.cm.Reds)\n",
    "#     plt.title(\"Correlation high target features Heatmap\")\n",
    "#     plt.show()\n",
    "# \n",
    "#     # Get the upper triangle of the correlation matrix\n",
    "#     upper_triangle = filtered_cor.where(np.triu(np.ones(filtered_cor.shape), k=1).astype(bool))\n",
    "# \n",
    "#     # Find indices where values are greater than 0.5\n",
    "#     indices = [(i, j) for i in range(filtered_cor.shape[0]-1) for j in range(i + 1, filtered_cor.shape[1]-1) if abs(upper_triangle.iloc[i, j]) > 0.5]\n",
    "#     #print(indices)\n",
    "#     # Display the list of indices\n",
    "#     indices = [list(pair) for pair in indices]\n",
    "#     #print(indices)\n",
    "# \n",
    "#     #compare input features that correlate with each other and select the one that has the highest correlation with the target variable\n",
    "#     array = []\n",
    "#     for i in range(len(indices)):\n",
    "#         if relevant_features[indices[i][0]] > relevant_features[indices[i][1]]:\n",
    "#             array.append(indices[i][0])\n",
    "#         else:\n",
    "#             array.append(indices[i][1])\n",
    "# \n",
    "#     # [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [4, 5], [4, 8], [5, 8], [6, 7], [6, 8], [7, 8]]\n",
    "#     #print('array',array)\n",
    "# \n",
    "#     # 1, 2, 3, 4, 5, 6, 7, 8, 2, 3, 4, 5, 6, 7, 8, 3, 4, 5, 6, 7, 8, 4, 5, 6, 7, 8, 5, 8, 8, 7, 8, 8\n",
    "#     array = np.unique(array)\n",
    "#     #print('uniqe_array',array)\n",
    "#     final_features= relevant_features.index[array]\n",
    "#     #print('final_features',final_features)\n",
    "# \n",
    "#     final_data = [df[final_features] for df in data]\n",
    "#     final_data.append(final_features)\n",
    "# \n",
    "#     # temp = train_data[0]\n",
    "#     # train_data_final = temp[feature_names]\n",
    "#     # train_data_final['pv_measurements'] = target\n",
    "#     # train_data_final['date_forecast'] = temp['date_forecast']\n",
    "#     # column_order = ['date_forecast'] + [col for col in train_data_final.columns if col != 'date_forecast']\n",
    "#     # train_data_final = train_data_final[column_order]\n",
    "#     # \n",
    "#     # test_data_final = train_data[2[feature_names]]\n",
    "#     # test_data_final = test_data_final[test_data_final.iloc[:,1:].notnull().all(axis=1)]\n",
    "# \n",
    "# \n",
    "#     #return the following final_data = [final_train_observed, final_train_estimated,final_test_estimated, feature_names]\n",
    "#     return final_data\n",
    "# \n",
    "# #Apply person correlation and extract important features\n",
    "# final_data_a = correlation_matrix(preprocessed_A)\n",
    "# final_data_b = correlation_matrix(preprocessed_B)\n",
    "# final_data_c = correlation_matrix(preprocessed_C)\n",
    "# \n",
    "# #add pv_measurement back to observed train data\n",
    "# final_data_a[0]['pv_measurements'] = training_A[1]\n",
    "# final_data_a[0].to_excel('final_train_observed_A.xlsx')\n",
    "# final_data_b[0]['pv_measurements'] = training_B[1]\n",
    "# final_data_b[0].to_excel('final_train_observed_B.xlsx')\n",
    "# final_data_c[0]['pv_measurements'] = training_C[1]\n",
    "# final_data_b[0].to_excel('final_train_observed_C.xlsx')\n",
    "\n",
    "#saving train observed data to csv\n",
    "# final_data_a[0].to_csv('final_train_observed_A.csv', index=False)\n",
    "# final_data_b[0].to_csv('final_train_observed_B.csv', index=False)\n",
    "# final_data_c[0].to_csv('final_train_observed_C.csv', index=False)\n",
    "\n",
    "#saving train estimated data to csv\n",
    "# final_data_a[1].to_csv('final_train_estimated_A.csv', index=False)\n",
    "# final_data_b[1].to_csv('final_train_estimated_B.csv', index=False)\n",
    "# final_data_c[1].to_csv('final_train_estimated_C.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5db1ad72f7ab747",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T12:08:30.490379200Z",
     "start_time": "2023-11-08T12:08:30.490379200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
